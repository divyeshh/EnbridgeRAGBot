# Production RAG Chatbot - FastAPI + Streamlit

A production-ready RAG (Retrieval-Augmented Generation) chatbot with FastAPI backend and Streamlit frontend.

## ğŸš€ Features

- âœ… **Free LLM**: Groq (Llama 3.1 8B) - 14,400 requests/day
- âœ… **Free Embeddings**: HuggingFace (sentence-transformers)
- âœ… **Document Upload**: PDF and DOCX support
- âœ… **Conversational**: Maintains chat history
- âœ… **REST API**: FastAPI backend with Swagger docs
- âœ… **Modern UI**: Streamlit frontend
- âœ… **Production Ready**: Modular architecture

## ğŸ“ Project Structure

```
RAG langchain chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ rag_core.py          # Core RAG functionality
â”‚   â””â”€â”€ uploaded_documents/  # Uploaded files
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py               # Streamlit UI
â”œâ”€â”€ chroma_db/               # Vector database
â”œâ”€â”€ .env                     # Environment variables
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Installation

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Environment

The `.env` file is already configured with your Groq API key.

## ğŸš€ Running the Application

### Option 1: Run Both (Recommended)

**Terminal 1 - Start Backend:**
```bash
cd backend
python main.py
```
Backend will run at: `http://localhost:8000`

**Terminal 2 - Start Frontend:**
```bash
cd frontend
streamlit run app.py
```
Frontend will open automatically in your browser

### Option 2: Run Separately

**Backend Only:**
```bash
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Frontend Only:**
```bash
cd frontend
streamlit run app.py
```

## ğŸ“– API Documentation

Once the backend is running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/status` | GET | System status |
| `/upload` | POST | Upload documents |
| `/chat` | POST | Chat with RAG |
| `/documents` | GET | List documents |
| `/documents/{filename}` | DELETE | Delete document |
| `/vectorstore` | DELETE | Clear all data |

### Example API Usage

**Upload Document:**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "files=@document.pdf"
```

**Chat:**
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is this document about?",
    "chat_history": []
  }'
```

## ğŸ¨ Using the Streamlit UI

1. **Upload Documents**:
   - Click "Browse files" in the sidebar
   - Select PDF or DOCX files
   - Click "Process Documents"

2. **Chat**:
   - Type your question in the chat input
   - View AI responses with source citations
   - Chat history is maintained automatically

3. **Manage Documents**:
   - View uploaded documents in sidebar
   - Delete individual documents
   - Clear all data with one click

## ğŸ”‘ Configuration

Edit `.env` to customize:

```env
# Change LLM model
LLM_MODEL=llama-3.1-8b-instant

# Change embedding model
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Change API port
API_PORT=8000
```

## ğŸ§ª Testing

**Test Backend:**
```bash
# Health check
curl http://localhost:8000/

# Check status
curl http://localhost:8000/status
```

**Test with Python:**
```python
import requests

# Upload document
files = {"files": open("document.pdf", "rb")}
response = requests.post("http://localhost:8000/upload", files=files)
print(response.json())

# Chat
response = requests.post(
    "http://localhost:8000/chat",
    json={"question": "What is this about?", "chat_history": []}
)
print(response.json())
```

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚   FastAPI    â”‚
â”‚   Frontend  â”‚                â”‚   Backend    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚  RAG Core    â”‚
                               â”‚  - Groq LLM  â”‚
                               â”‚  - HF Embed  â”‚
                               â”‚  - ChromaDB  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› Troubleshooting

**Port already in use:**
```bash
# Change port in .env or run with different port
uvicorn main:app --port 8001
```

**Cannot connect to API:**
- Ensure backend is running
- Check `API_URL` in frontend matches backend URL

**Module not found:**
```bash
pip install -r requirements.txt
```

**Groq API errors:**
- Check your API key in `.env`
- Verify you haven't exceeded rate limits (14,400/day)

## ğŸš€ Deployment

### Deploy Backend (FastAPI)

**Using Docker:**
```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY backend/ .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Using Railway/Render:**
- Deploy `backend/` directory
- Set environment variables from `.env`
- Start command: `uvicorn main:app --host 0.0.0.0 --port $PORT`

### Deploy Frontend (Streamlit)

**Streamlit Cloud:**
1. Push code to GitHub
2. Connect to Streamlit Cloud
3. Set `API_URL` to your backend URL

## ğŸ“ License

This project uses:
- Groq API (free tier)
- HuggingFace models (Apache 2.0)
- LangChain (MIT)

## ğŸ†˜ Support

- **Backend Issues**: Check `http://localhost:8000/docs`
- **Frontend Issues**: Check Streamlit terminal output
- **API Errors**: Enable LangSmith tracing in `.env`

---

**Built with â¤ï¸ using Groq, HuggingFace, FastAPI, and Streamlit**
