# RAG Chatbot with Groq & HuggingFace

A free, production-ready RAG (Retrieval-Augmented Generation) chatbot using:
- **Groq** for LLM (Llama 3.1 70B) - 14,400 free requests/day
- **HuggingFace** for embeddings (all-MiniLM-L6-v2) - 100% free, runs locally
- **LangChain** for orchestration
- **Chroma** for vector storage

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install langchain langchain-groq langchain-huggingface langchain-core langchain_community langchain_chroma docx2txt pypdf sentence_transformers
```

### 2. Set Up API Keys

Your Groq API key is already configured in the notebook. For production use:

```python
import os
os.environ["GROQ_API_KEY"] = "your-groq-api-key"
```

### 3. Run the Notebook

Open `LangChain_Conversational_RAG_Crash_Course_From_Basics_to_Production_Part_1.ipynb` and run all cells.

## ğŸ“Š API Keys

### Groq (Required)
- **Key**: `your_groq_api_key_here`
- **Free Tier**: 14,400 requests/day, 7,000 requests/minute
- **Model**: Llama 3.1 70B Versatile

### LangSmith (Optional - for debugging)
- **Key**: `your_langsmith_api_key_here`
- **Status**: Disabled by default (paid service)
- **Enable**: Set `LANGCHAIN_TRACING_V2=true` in environment

## ğŸ§ª Testing

Run the test script to verify everything works:

```bash
python test_groq_migration.py
```

Expected output:
```
âœ… Groq LLM: Working (Llama 3.1 70B)
âœ… HuggingFace Embeddings: Working (384 dimensions)
```

## ğŸ“ Project Structure

```
RAG langchain chatbot/
â”œâ”€â”€ LangChain_Conversational_RAG_Crash_Course_From_Basics_to_Production_Part_1.ipynb  # Main notebook
â”œâ”€â”€ migrate_to_groq.py           # Migration script
â”œâ”€â”€ test_groq_migration.py       # Test script
â”œâ”€â”€ .env.example                 # API keys template
â””â”€â”€ README.md                    # This file
```

## ğŸ’¡ Features

- âœ… **100% Free**: No API costs with Groq free tier
- âœ… **Fast**: Groq provides ~500 tokens/second
- âœ… **Local Embeddings**: HuggingFace runs on your machine
- âœ… **Production Ready**: Full RAG pipeline with vector storage
- âœ… **Conversational**: Handles follow-up questions with context

## ğŸ”§ Configuration

### Enable LangSmith Monitoring (Optional)

If you want to monitor your LLM calls for debugging:

```python
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_api_key_here"
os.environ["LANGCHAIN_PROJECT"] = "rag-chatbot-dev"
```

**Note**: LangSmith is a paid service. Only enable if needed for development.

## ğŸ“š Documentation

See `walkthrough.md` in the artifacts folder for detailed migration documentation.

## ğŸ¯ Next Steps

1. **Add Your Documents**: Place PDF/DOCX files in a `docs/` folder
2. **Customize Prompts**: Modify the system prompts in the notebook
3. **Deploy**: Use FastAPI to create a REST API (see Part 2 of the course)
4. **Build UI**: Create a Streamlit interface for end users

## ğŸ†˜ Troubleshooting

### "Module not found" errors
```bash
pip install langchain-groq langchain-huggingface sentence-transformers
```

### Slow first run
HuggingFace downloads the embedding model (~80MB) on first run. Subsequent runs are fast.

### Rate limits
Groq free tier: 14,400 requests/day. If exceeded, wait 24 hours or upgrade.

## ğŸ“ License

This project uses:
- Groq API (free tier)
- HuggingFace models (Apache 2.0)
- LangChain (MIT)
